<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Loss functions in Deep Learning | Shreyas Pimpalgaonkar </title> <meta name="author" content="Shreyas Pimpalgaonkar"> <meta name="description" content="Short introduction to policy gradient methods"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/logo.png?475abb3e5d97a755ee05c96fc895ea2c"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://shreyaspimpalgaonkar.github.io/blog/2024/dl-losses/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?bf50d6d9dd867d3e0f3b0add94449649"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="//"> Shreyas Pimpalgaonkar </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/projects/">projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Loss functions in Deep Learning</h1> <p class="post-meta"> March 14, 2024 </p> <p class="post-tags"> <a href="//blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="//blog/tag/loss-functions"> <i class="fa-solid fa-hashtag fa-sm"></i> Loss Functions</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="introduction">Introduction</h2> <p>In this post, we will cover many popular loss functions used in deep learning. We will cover the whats, whys, hows, pros, cons and use cases of each loss function. I will keep adding new loss functions to this post as I come across them.</p> <h2 id="notation">Notation</h2> <p>We will use the following notation throughout this post:</p> <ul> <li>\(x_i\): the input features</li> <li>\(\hat{y}_i\): the predicted values</li> <li>\(y_i\): the actual values</li> <li>\(n\): the number of samples</li> </ul> <h3 id="mean-squared-error-mse">Mean Squared Error (MSE)</h3> <p>Tags: Regression</p> <p>What: The mean squared error (MSE) is a loss function that is commonly used in regression problems. It is the average of the squared differences between the predicted and actual values. The MSE is defined as:</p> \[MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2\] <p>Why: It directly corresponds to distance in the euclidean space. It is a good loss function to use when the errors are normally distributed and the outliers are not a big concern.</p> <p>How: In practice, MSE is used as a criterion to train a model. The model’s parameters are adjusted to minimize the MSE. During training, the MSE is computed for each sample, and the average MSE is used as the loss function. During evaluation, the MSE is computed for each sample, and the average MSE is used as the performance metric.</p> <p>Pros:</p> <ul> <li>Differentiable</li> <li>Easy to interpret</li> <li>Possible to compute analytical gradient</li> <li>Ensures convergence to a global minimum in convex optimization problems</li> </ul> <p>Cons:</p> <ul> <li>Sensitive to outliers</li> <li>Not always indicative of the model’s performance. For example, if the model is predicting a robot’s position, the MSE might be low, but the robot might still be crashing into walls.</li> </ul> <h3 id="mean-absolute-error-mae">Mean Absolute Error (MAE)</h3> <p>Tags: Regression</p> <p>What: The mean absolute error (MAE) is a loss function that is commonly used in regression problems. It is the average of the absolute differences between the predicted and actual values. The MAE is defined as:</p> \[MAE = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|\] <p>Why: The MAE is a good loss function to use when the outliers are a big concern.</p> <p>Pros:</p> <ul> <li>Differentiable</li> <li>Robust to outliers</li> </ul> <p>Cons:</p> <ul> <li>Cannot be used when large errors should be penalized more than small errors</li> <li>Large gradients near zero, which can make training difficult</li> <li>Non differentiable at zero, which can be problematic for gradient-based optimization algorithms</li> </ul> <h3 id="huber-loss">Huber Loss</h3> <p>Tags: Regression</p> <p>What: The Huber loss is a loss function that is a combination of the MSE and MAE. It is less sensitive to outliers than the MSE and less sensitive to small errors than the MAE. The Huber loss is defined as:</p> \[L_{\delta} = \frac{1}{n} \sum_{i=1}^{n} \begin{cases} \frac{1}{2}(y_i - \hat{y}_i)^2, &amp; \text{if } |y_i - \hat{y}_i| \leq \delta \\ \delta |y_i - \hat{y}_i| - \frac{1}{2}\delta^2, &amp; \text{otherwise} \end{cases}\] <p>Beyond the threshold \(\delta\), the loss function behaves like the MAE, and below the threshold \(\delta\), the loss function behaves like the MSE.</p> <p>where \(\delta\) is a hyperparameter that controls the threshold between the MSE and MAE.</p> <p>Why: The Huber loss is a good loss function to use when the errors are not normally distributed and the outliers are a concern.</p> <p>Pros:</p> <ul> <li>Differentiable</li> <li>Robust to outliers</li> <li>Less sensitive to outliers than the MSE and less sensitive to small errors than the MAE</li> </ul> <p>Cons:</p> <ul> <li>More complex to compute than MSE and MAE</li> <li>Hyperparameter \(\delta\) needs to be tuned</li> </ul> <h3 id="binary-cross-entropy-loss">Binary Cross-Entropy Loss</h3> <p>Tags: Classification</p> <p>What: The binary cross-entropy loss is a loss function that is commonly used in binary classification problems. It is the negative log-likelihood of the true labels given the predicted probabilities. The binary cross-entropy loss is defined as:</p> \[BCE = -\frac{1}{n} \sum_{i=1}^{n} y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)\] <p>where \(y_i\) is either 0 or 1, and \(\hat{y}_i\) is the predicted probability of the positive class.</p> <p>Why: Naturally suited for classification tasks as it penalises model outputs from how divergent they are from the true labels. It’s interpretation is also intuitive as it is the negative log-likelihood of the true labels given the predicted probabilities.</p> <p>How: We use BCE as a criterion to train a model. The model’s parameters are adjusted to align predictions with the true labels. During training, the BCE is computed for each sample, and the average BCE is used as the loss function. During evaluation, the BCE is computed for each sample, and the average BCE is used as the performance metric.</p> <p>Pros:</p> <ul> <li>Sensitivity to probability distributions</li> <li>Differentiable and stable gradients</li> <li>Direct optimization of classification performance</li> </ul> <p>Cons:</p> <ul> <li>Sensitive to class imbalance</li> <li>Numerical instability when the predicted probabilities are close to 0 or 1</li> <li>Not suitable for multi-class classification</li> </ul> <h3 id="categorical-cross-entropy-loss">Categorical Cross-Entropy Loss</h3> <p>Tags: Classification</p> <p>What: The categorical cross-entropy loss is a loss function that is commonly used in multi-class classification problems. It is the negative log-likelihood of the true labels given the predicted probabilities. The categorical cross-entropy loss is defined as:</p> \[CCE = -\frac{1}{n} \sum_{i=1}^{n} \sum_{j=1}^{m} y_{ij} \log(\hat{y}_{ij})\] <p>where \(y_{ij}\) is either 0 or 1, and \(\hat{y}_{ij}\) is the predicted probability of the \(j\)-th class.</p> <p>Why: Naturally suited for multi-class classification tasks as it penalises model outputs from how divergent they are from the true labels. It’s interpretation is also intuitive as it is the negative log-likelihood of the true labels given the predicted probabilities.</p> <p>Pros:</p> <ul> <li>Effective for multi-class classification</li> <li>Differentiable and stable gradients</li> <li>Direct optimization of classification performance</li> </ul> <p>Cons:</p> <ul> <li>Sensitive to class imbalance</li> <li>Requires one-hot encoding of the true labels</li> <li>Numerical instability when the predicted probabilities are close to 0 or 1</li> </ul> <h3 id="sparse-categorical-cross-entropy-loss">Sparse Categorical Cross-Entropy Loss</h3> <p>Tags: Classification, Computer Vision</p> <p>What: Loss function similar to CCE but used when the true labels are not one-hot encoded.</p> <h3 id="hinge-loss">Hinge Loss</h3> <p>Tags: Classification</p> <p>What: The hinge loss is a loss function that is commonly used in binary classification problems to classify with maximum margin, mainly SVMs. It is the maximum of 0 and the difference between 1 and the predicted value times the true label. The hinge loss is defined as:</p> \[HL = \frac{1}{n} \sum_{i=1}^{n} \max(0, 1 - y_i \hat{y}_i)\] <p>where \(y_i\) is either -1 or 1, and \(\hat{y}_i\) is the predicted value. The loss penalizes the model when the predicted value is not in the correct direction of the true label. Even if the predicted value is correct, the loss is still penalized if the predicted value is not far enough from the decision boundary. An extension of the hinge loss is the squared hinge loss, which squares the difference between 1 and the predicted value times the true label.</p> <p>Why: Suited for classification tasks as it encourages the model to classify with maximum margin, which gives robust classification boundaries.</p> <p>Pros:</p> <ul> <li>Encourages the model to classify with maximum margin, gives robust classification boundaries</li> <li>Solution relies on the support vectors (sparse subset of the data), which makes the model more robust to outliers</li> </ul> <p>Cons:</p> <ul> <li>Non probabilistic, difficult to interpret</li> <li>Not easily extendable to multi-class classification</li> </ul> <h3 id="focal-loss">Focal Loss</h3> <p>Tags: Classification, Object Detection</p> <p>What: The focal loss is a loss function that is commonly used in binary classification problems. It is a modification of the binary cross-entropy loss that down-weights the contribution of well-classified examples. The focal loss is defined as:</p> \[FL = -\frac{1}{n} \sum_{i=1}^{n} \alpha_t (1 - \hat{y}_i)^\gamma \log(\hat{y}_i)\] <p>where \(\alpha_t\) is a hyperparameter that controls class imbalance (tuned using cross validation), \(\gamma\) is a hyperparameter that controls the down-weighting of well-classified examples. \(\gamma\) is typically set to 2.</p> <p>Why: The focal loss is a good loss function to use when the class imbalance is a concern. It down-weights the contribution of well-classified examples, which makes the model focus more on the hard examples.</p> <p>Pros:</p> <ul> <li>Effective for class imbalance</li> <li>Improved learning on difficult examples</li> <li>Can be used for multi-class classification</li> </ul> <p>Cons:</p> <ul> <li>Requires tuning of hyperparameters \(\alpha_t\) and \(\gamma\)</li> <li>performance is sensitive to these hyperparameters</li> <li>Similar effectiveness to CE when class imbalance is not a concern</li> </ul> <p>Links:</p> <ul> <li><a href="https://arxiv.org/pdf/1708.02002.pdf" rel="external nofollow noopener" target="_blank">Focal Loss for Dense Object Detection</a></li> </ul> <h3 id="kullback-leibler-divergence-kl-divergence">Kullback-Leibler Divergence (KL Divergence)</h3> <p>Tags: Generative Models, VAE, GAN</p> <p>What: The Kullback-Leibler divergence (KL divergence) is a loss function that is commonly used in generative models. It is a measure from information theory that measures how one probability distribution diverges from a second, approximate probability distribution. The KL divergence is defined as:</p> \[KL(P||Q) = \sum_{x \in \mathcal{X}} P(x) \log \frac{P(x)}{Q(x)}\] <p>where \(P\) is the true distribution and \(Q\) is the approximate distribution. It is not symmetric and is always non-negative. It is zero if and only if \(P\) and \(Q\) are the same distribution. It is infinite if the support of \(P\) is not contained in the support of \(Q\).</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="//blog/2024/LLM-inference-optimizations/">LLM Inference Optimization Techniques</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="//blog/2024/policy_gradient/">Policy Gradient</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="//blog/2024/AutoContext/">AutoContext</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="//blog/2024/TDMPC2/">TDMPC-2</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="//blog/2024/Interesting-Links/">Interesting Links</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Shreyas Pimpalgaonkar. Last updated: March 29, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?7254ae07fe9cc5f3a10843e1c0817c9c" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>