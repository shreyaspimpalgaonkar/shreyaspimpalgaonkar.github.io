--- 
layout: post
title: LLM Inference Optimization Techniques 
date: 2024-03-27
description: Inference go brrrrr
tags: [LLM, RAG]
---

## Introduction

We have seen a lot of progress in the field of large language models (LLMs) in the past few years. The models have become larger and more powerful, but the inference time has also increased. This has led to the development of various techniques to optimize the inference time of LLMs. In this blog post, we will discuss some of the techniques that are commonly used to optimize the inference time of LLMs.

## List of LLM Inference Optimization Startups

[This](https://leaderboard.withmartian.com) website lists top companies in the LLM inference hosting space. A screenshot is below:

<!-- image with 100% width -->

<div style="text-align:center; padding-top: 20px; padding-bottom: 40px;">
    <img src="/assets/img/blogs/llm-inference/leaderboard.png" alt="LLM Inference Optimization Leaderboard" width="100%">


There are a lot more companies. More companies below. 

1. NVIDIA
2. Groq
3. Lightning
4. d-matrix
5. Hugging Face
6. SambaNova
7. Graphcore
8. Cerebras
9. Tenstorrent
10. Mythic
11. Flex Logix
12. Wave Computing
13. Esperanto
14. Syntiant
15. DeepInfra
16. Kneron
17. Blaize
18. Flex Logix
19. Abacus