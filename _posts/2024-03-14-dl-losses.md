--- 
layout: post
title: Loss functions in Deep Learning
date: 2024-03-14
description: Short introduction to policy gradient methods
tags: [Loss Functions]
---

## Introduction

In this post, we will cover many popular loss functions used in deep learning. We will cover the whats, whys, hows, pros, cons and use cases of each loss function. I will keep adding new loss functions to this post as I come across them.

## Notation

We will use the following notation throughout this post:

- $$x_i$$: the input features
- $$\hat{y}_i$$: the predicted values
- $$y_i$$: the actual values
- $$n$$: the number of samples


### Mean Squared Error (MSE)

Tags: Regression

What: The mean squared error (MSE) is a loss function that is commonly used in regression problems. It is the average of the squared differences between the predicted and actual values. The MSE is defined as:

$$
MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

Why: It directly corresponds to distance in the euclidean space. It is a good loss function to use when the errors are normally distributed and the outliers are not a big concern. 

How: In practice, MSE is used as a criterion to train a model. The model's parameters are adjusted to minimize the MSE. During training, the MSE is computed for each sample, and the average MSE is used as the loss function. During evaluation, the MSE is computed for each sample, and the average MSE is used as the performance metric.

Pros: 
- Differentiable
- Easy to interpret
- Possible to compute analytical gradient 
- Ensures convergence to a global minimum in convex optimization problems

Cons: 
- Sensitive to outliers
- Not always indicative of the model's performance. For example, if the model is predicting a robot's position, the MSE might be low, but the robot might still be crashing into walls.


### Mean Absolute Error (MAE)

Tags: Regression

What: The mean absolute error (MAE) is a loss function that is commonly used in regression problems. It is the average of the absolute differences between the predicted and actual values. The MAE is defined as:

$$
MAE = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
$$

Why: The MAE is a good loss function to use when the outliers are a big concern.

Pros:
- Differentiable
- Robust to outliers

Cons:
- Cannot be used when large errors should be penalized more than small errors
- Large gradients near zero, which can make training difficult
- Non differentiable at zero, which can be problematic for gradient-based optimization algorithms

### Huber Loss

Tags: Regression

What: The Huber loss is a loss function that is a combination of the MSE and MAE. It is less sensitive to outliers than the MSE and less sensitive to small errors than the MAE. The Huber loss is defined as:

$$
L_{\delta} = \frac{1}{n} \sum_{i=1}^{n}
\begin{cases}
\frac{1}{2}(y_i - \hat{y}_i)^2, & \text{if } |y_i - \hat{y}_i| \leq \delta \\
\delta |y_i - \hat{y}_i| - \frac{1}{2}\delta^2, & \text{otherwise}
\end{cases}
$$

Beyond the threshold $$\delta$$, the loss function behaves like the MAE, and below the threshold $$\delta$$, the loss function behaves like the MSE.

where $$\delta$$ is a hyperparameter that controls the threshold between the MSE and MAE.

Why: The Huber loss is a good loss function to use when the errors are not normally distributed and the outliers are a concern. 

Pros:
- Differentiable
- Robust to outliers
- Less sensitive to outliers than the MSE and less sensitive to small errors than the MAE

Cons:
- More complex to compute than MSE and MAE
- Hyperparameter $$\delta$$ needs to be tuned


### Binary Cross-Entropy Loss

Tags: Classification

What: The binary cross-entropy loss is a loss function that is commonly used in binary classification problems. It is the negative log-likelihood of the true labels given the predicted probabilities. The binary cross-entropy loss is defined as:

$$
BCE = -\frac{1}{n} \sum_{i=1}^{n} y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)
$$

where $$y_i$$ is either 0 or 1, and $$\hat{y}_i$$ is the predicted probability of the positive class.

Why: Naturally suited for classification tasks as it penalises model outputs from how divergent they are from the true labels. It's interpretation is also intuitive as it is the negative log-likelihood of the true labels given the predicted probabilities.

How: We use BCE as a criterion to train a model. The model's parameters are adjusted to align predictions with the true labels. During training, the BCE is computed for each sample, and the average BCE is used as the loss function. During evaluation, the BCE is computed for each sample, and the average BCE is used as the performance metric.


Pros:
- Sensitivity to probability distributions
- Differentiable and stable gradients
- Direct optimization of classification performance

Cons:
- Sensitive to class imbalance
- Numerical instability when the predicted probabilities are close to 0 or 1
- Not suitable for multi-class classification


### Categorical Cross-Entropy Loss

Tags: Classification

What: The categorical cross-entropy loss is a loss function that is commonly used in multi-class classification problems. It is the negative log-likelihood of the true labels given the predicted probabilities. The categorical cross-entropy loss is defined as:

$$
CCE = -\frac{1}{n} \sum_{i=1}^{n} \sum_{j=1}^{m} y_{ij} \log(\hat{y}_{ij})
$$

where $$y_{ij}$$ is either 0 or 1, and $$\hat{y}_{ij}$$ is the predicted probability of the $$j$$-th class.

Why: Naturally suited for multi-class classification tasks as it penalises model outputs from how divergent they are from the true labels. It's interpretation is also intuitive as it is the negative log-likelihood of the true labels given the predicted probabilities.

Pros:
- Effective for multi-class classification
- Differentiable and stable gradients
- Direct optimization of classification performance

Cons: 
- Sensitive to class imbalance
- Requires one-hot encoding of the true labels
- Numerical instability when the predicted probabilities are close to 0 or 1

### Sparse Categorical Cross-Entropy Loss

Tags: Classification, Computer Vision

What: Loss function similar to CCE but used when the true labels are not one-hot encoded.


### Hinge Loss

Tags: Classification

What: The hinge loss is a loss function that is commonly used in binary classification problems to classify with maximum margin, mainly SVMs. It is the maximum of 0 and the difference between 1 and the predicted value times the true label. The hinge loss is defined as:

$$
HL = \frac{1}{n} \sum_{i=1}^{n} \max(0, 1 - y_i \hat{y}_i)
$$

where $$y_i$$ is either -1 or 1, and $$\hat{y}_i$$ is the predicted value. The loss penalizes the model when the predicted value is not in the correct direction of the true label. Even if the predicted value is correct, the loss is still penalized if the predicted value is not far enough from the decision boundary. An extension of the hinge loss is the squared hinge loss, which squares the difference between 1 and the predicted value times the true label.

Why: Suited for classification tasks as it encourages the model to classify with maximum margin, which gives robust classification boundaries.

Pros:
- Encourages the model to classify with maximum margin, gives robust classification boundaries
- Solution relies on the support vectors (sparse subset of the data), which makes the model more robust to outliers

Cons:
- Non probabilistic, difficult to interpret
- Not easily extendable to multi-class classification


### Focal Loss

Tags: Classification, Object Detection

What: The focal loss is a loss function that is commonly used in binary classification problems. It is a modification of the binary cross-entropy loss that down-weights the contribution of well-classified examples. The focal loss is defined as:

$$
FL = -\frac{1}{n} \sum_{i=1}^{n} \alpha_t (1 - \hat{y}_i)^\gamma \log(\hat{y}_i)
$$

where $$\alpha_t$$ is a hyperparameter that controls class imbalance (tuned using cross validation), $$\gamma$$ is a hyperparameter that controls the down-weighting of well-classified examples. $$\gamma$$ is typically set to 2.

Why: The focal loss is a good loss function to use when the class imbalance is a concern. It down-weights the contribution of well-classified examples, which makes the model focus more on the hard examples.

Pros:
- Effective for class imbalance
- Improved learning on difficult examples
- Can be used for multi-class classification

Cons:
- Requires tuning of hyperparameters $$\alpha_t$$ and $$\gamma$$
- performance is sensitive to these hyperparameters
- Similar effectiveness to CE when class imbalance is not a concern

Links: 
- [Focal Loss for Dense Object Detection](https://arxiv.org/pdf/1708.02002.pdf)


### Kullback-Leibler Divergence (KL Divergence)

Tags: Generative Models, VAE, GAN

What: The Kullback-Leibler divergence (KL divergence) is a loss function that is commonly used in generative models. It is a measure from information theory that measures how one probability distribution diverges from a second, approximate probability distribution. The KL divergence is defined as:

$$
KL(P||Q) = \sum_{x \in \mathcal{X}} P(x) \log \frac{P(x)}{Q(x)}
$$

where $$P$$ is the true distribution and $$Q$$ is the approximate distribution. It is not symmetric and is always non-negative. It is zero if and only if $$P$$ and $$Q$$ are the same distribution. It is infinite if the support of $$P$$ is not contained in the support of $$Q$$.



