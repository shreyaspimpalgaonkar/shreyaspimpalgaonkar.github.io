<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://shreyaspimpalgaonkar.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://shreyaspimpalgaonkar.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-03-29T04:50:52+00:00</updated><id>https://shreyaspimpalgaonkar.github.io/feed.xml</id><title type="html">Shreyas Pimpalgaonkar</title><entry><title type="html">LLM Inference Optimization Techniques</title><link href="https://shreyaspimpalgaonkar.github.io/blog/2024/LLM-inference-optimizations/" rel="alternate" type="text/html" title="LLM Inference Optimization Techniques"/><published>2024-03-27T00:00:00+00:00</published><updated>2024-03-27T00:00:00+00:00</updated><id>https://shreyaspimpalgaonkar.github.io/blog/2024/LLM-inference-optimizations</id><content type="html" xml:base="https://shreyaspimpalgaonkar.github.io/blog/2024/LLM-inference-optimizations/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>We have seen a lot of progress in the field of large language models (LLMs) in the past few years. The models have become larger and more powerful, but the inference time has also increased. This has led to the development of various techniques to optimize the inference time of LLMs. In this blog post, we will discuss some of the techniques that are commonly used to optimize the inference time of LLMs.</p> <h2 id="list-of-llm-inference-optimization-startups">List of LLM Inference Optimization Startups</h2> <p><a href="https://leaderboard.withmartian.com">This</a> website lists top companies in the LLM inference hosting space. A screenshot is below:</p> <div style="text-align:center; padding-top: 20px; padding-bottom: 40px;"> <img src="/assets/img/blogs/llm-inference/leaderboard.png" alt="LLM Inference Optimization Leaderboard" width="100%"/> There are a lot more companies. More companies below. - NVIDIA - Groq - Lightning - d-matrix - Hugging Face - SambaNova - Graphcore - Cerebras - Tenstorrent - Mythic - Flex Logix - Wave Computing - Esperanto - Syntiant - DeepInfra - Kneron - Blaize - Flex Logix - Abacus ## Techniques for LLM Inference Optimization Algorithmic, SOftware and Hardware optimizations are three main categoeis ### Algorithmic Optimizations - More Efficient Models like MQA/GQA vs MHA, using less transformer layers - S4 Models - ### Runtime level optimizations - KV Caching: Durnig the decoding phase, each token output needs to calculate the tensors for KV pairs for all previous tokens. This can be optimized by caching the KV pairs for each token and reusing them while generating the next token. This reduces the number of multiplications required and improve the inference time. Size of KV cache = `2 * batch_size * sequence_length * num_layers * hidden_size * size_of_fp16` We can see that the size of the cache is linear in sequence length and batch size. It is a challenge for long context models. - Kernel Fusion: - Tensor Layout Optimization: - Pipeline Parallelism: (GPipe)[https://arxiv.org/pdf/1811.06965.pdf] divides the model into multiple devices. It divides each mini-batch into micro batches and processes them in a pipeline fashion. <div style="text-align:center; padding-top: 20px; padding-bottom: 40px;"> <img src="/assets/img/blogs/llm-inference/gpipe.png" alt="Gpipe Pipeline Parallelism" width="100%"/> - Flash Attention - Paged Attention - TGI - Citations: [1] https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/ [2] https://deci.ai/resources/webinar-llms-at-scale-top-inference-optimization-libraries/ [3] https://arxiv.org/pdf/1811.06965.pdf [4] https://nyu-mlsys.github.io/schedule.html </div></div>]]></content><author><name></name></author><category term="LLM"/><category term="RAG"/><summary type="html"><![CDATA[Inference go brrrrr]]></summary></entry><entry><title type="html">AutoContext</title><link href="https://shreyaspimpalgaonkar.github.io/blog/2024/AutoContext/" rel="alternate" type="text/html" title="AutoContext"/><published>2024-03-24T00:00:00+00:00</published><updated>2024-03-24T00:00:00+00:00</updated><id>https://shreyaspimpalgaonkar.github.io/blog/2024/AutoContext</id><content type="html" xml:base="https://shreyaspimpalgaonkar.github.io/blog/2024/AutoContext/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>In this post, we will cover a tool called AutoContext that me and my team developed at IvyHacks 2024. We won the SciPhi sponsor prize for this project.</p> <h2 id="use-cases">Use Cases</h2> <ul> <li> <p>Youâ€™re a programmer and you are browsing obscure documentation on the internet. You ask ChatGPT questions about the documentation but it has no idea. You copy relevant parts of the documentation and paste it into ChatGPTâ€™s window and ask it questions again. Hopefully, ChatGPT will be able to answer your questions this time. But it is tedious to optimize what to copy-paste while still being in the context limit. It is not scalable and manual leading to suboptimal interactions with the LLM.</p> </li> <li> <p>You are a researcher and you are browsing papers on the internet. You ask ChatGPT questions about the papers but it has no idea. You copy relevant parts of the papers and paste it into ChatGPTâ€™s window and ask it questions again. Hopefully, ChatGPT will be able to answer your questions this time. But it is tedious to optimize what to copy-paste while still being in the context limit. It is not scalable and manual leading to suboptimal interactions with the LLM.</p> </li> </ul> <p>There are many more such use cases where ChatGPT needs context to answer questions. AutoContext is a tool that helps you provide context to ChatGPT in a scalable and automated way. Not only it works with ChatGPT but it can be used with any LLM.</p> <h2 id="how-it-works">How it works</h2> <p>Simply just install the chrome extension and browse the internet. The extension will automatically store the content into its memory without you having to do anything. We define context into three parts: short term, long term and personal.</p> <div style="text-align:center; padding-top: 20px; padding-bottom: 20px;"> <img src="/assets/img/blogs/autocontext/memory.png" alt="AutoContext Memory" width="50%"/> </div> <p>Now, when you ask LLM a question, the extension will automatically fetch the relevant context from its memory and paste it into the LLMâ€™s window. This way you can ask questions to LLM without worrying about fetching the content. This is what it looks like:</p> <div style="text-align:center; padding-top: 20px; padding-bottom: 40px;"> <img src="/assets/img/blogs/autocontext/claude.png" alt="AutoContext Demo" width="100%"/> </div> <h2 id="final-thoughts">Final Thoughts</h2> <p>Weâ€™re excited to be working on this project and we believe it will help a lot of people. Weâ€™re still finalizing some parts and will be releasing it soon. Shoot me an email if you have any questions! Stay tuned for more updates ðŸ¦¾</p>]]></content><author><name></name></author><category term="LLM"/><category term="RAG"/><summary type="html"><![CDATA[Automatically keep context for your LLM interactions]]></summary></entry><entry><title type="html">Loss functions in Deep Learning</title><link href="https://shreyaspimpalgaonkar.github.io/blog/2024/dl-losses/" rel="alternate" type="text/html" title="Loss functions in Deep Learning"/><published>2024-03-14T00:00:00+00:00</published><updated>2024-03-14T00:00:00+00:00</updated><id>https://shreyaspimpalgaonkar.github.io/blog/2024/dl-losses</id><content type="html" xml:base="https://shreyaspimpalgaonkar.github.io/blog/2024/dl-losses/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>In this post, we will cover many popular loss functions used in deep learning. We will cover the whats, whys, hows, pros, cons and use cases of each loss function. I will keep adding new loss functions to this post as I come across them.</p> <h2 id="notation">Notation</h2> <p>We will use the following notation throughout this post:</p> <ul> <li>\(x_i\): the input features</li> <li>\(\hat{y}_i\): the predicted values</li> <li>\(y_i\): the actual values</li> <li>\(n\): the number of samples</li> </ul> <h3 id="mean-squared-error-mse">Mean Squared Error (MSE)</h3> <p>Tags: Regression</p> <p>What: The mean squared error (MSE) is a loss function that is commonly used in regression problems. It is the average of the squared differences between the predicted and actual values. The MSE is defined as:</p> \[MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2\] <p>Why: It directly corresponds to distance in the euclidean space. It is a good loss function to use when the errors are normally distributed and the outliers are not a big concern.</p> <p>How: In practice, MSE is used as a criterion to train a model. The modelâ€™s parameters are adjusted to minimize the MSE. During training, the MSE is computed for each sample, and the average MSE is used as the loss function. During evaluation, the MSE is computed for each sample, and the average MSE is used as the performance metric.</p> <p>Pros:</p> <ul> <li>Differentiable</li> <li>Easy to interpret</li> <li>Possible to compute analytical gradient</li> <li>Ensures convergence to a global minimum in convex optimization problems</li> </ul> <p>Cons:</p> <ul> <li>Sensitive to outliers</li> <li>Not always indicative of the modelâ€™s performance. For example, if the model is predicting a robotâ€™s position, the MSE might be low, but the robot might still be crashing into walls.</li> </ul> <h3 id="mean-absolute-error-mae">Mean Absolute Error (MAE)</h3> <p>Tags: Regression</p> <p>What: The mean absolute error (MAE) is a loss function that is commonly used in regression problems. It is the average of the absolute differences between the predicted and actual values. The MAE is defined as:</p> \[MAE = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|\] <p>Why: The MAE is a good loss function to use when the outliers are a big concern.</p> <p>Pros:</p> <ul> <li>Differentiable</li> <li>Robust to outliers</li> </ul> <p>Cons:</p> <ul> <li>Cannot be used when large errors should be penalized more than small errors</li> <li>Large gradients near zero, which can make training difficult</li> <li>Non differentiable at zero, which can be problematic for gradient-based optimization algorithms</li> </ul> <h3 id="huber-loss">Huber Loss</h3> <p>Tags: Regression</p> <p>What: The Huber loss is a loss function that is a combination of the MSE and MAE. It is less sensitive to outliers than the MSE and less sensitive to small errors than the MAE. The Huber loss is defined as:</p> \[L_{\delta} = \frac{1}{n} \sum_{i=1}^{n} \begin{cases} \frac{1}{2}(y_i - \hat{y}_i)^2, &amp; \text{if } |y_i - \hat{y}_i| \leq \delta \\ \delta |y_i - \hat{y}_i| - \frac{1}{2}\delta^2, &amp; \text{otherwise} \end{cases}\] <p>Beyond the threshold \(\delta\), the loss function behaves like the MAE, and below the threshold \(\delta\), the loss function behaves like the MSE.</p> <p>where \(\delta\) is a hyperparameter that controls the threshold between the MSE and MAE.</p> <p>Why: The Huber loss is a good loss function to use when the errors are not normally distributed and the outliers are a concern.</p> <p>Pros:</p> <ul> <li>Differentiable</li> <li>Robust to outliers</li> <li>Less sensitive to outliers than the MSE and less sensitive to small errors than the MAE</li> </ul> <p>Cons:</p> <ul> <li>More complex to compute than MSE and MAE</li> <li>Hyperparameter \(\delta\) needs to be tuned</li> </ul> <h3 id="binary-cross-entropy-loss">Binary Cross-Entropy Loss</h3> <p>Tags: Classification</p> <p>What: The binary cross-entropy loss is a loss function that is commonly used in binary classification problems. It is the negative log-likelihood of the true labels given the predicted probabilities. The binary cross-entropy loss is defined as:</p> \[BCE = -\frac{1}{n} \sum_{i=1}^{n} y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)\] <p>where \(y_i\) is either 0 or 1, and \(\hat{y}_i\) is the predicted probability of the positive class.</p> <p>Why: Naturally suited for classification tasks as it penalises model outputs from how divergent they are from the true labels. Itâ€™s interpretation is also intuitive as it is the negative log-likelihood of the true labels given the predicted probabilities.</p> <p>How: We use BCE as a criterion to train a model. The modelâ€™s parameters are adjusted to align predictions with the true labels. During training, the BCE is computed for each sample, and the average BCE is used as the loss function. During evaluation, the BCE is computed for each sample, and the average BCE is used as the performance metric.</p> <p>Pros:</p> <ul> <li>Sensitivity to probability distributions</li> <li>Differentiable and stable gradients</li> <li>Direct optimization of classification performance</li> </ul> <p>Cons:</p> <ul> <li>Sensitive to class imbalance</li> <li>Numerical instability when the predicted probabilities are close to 0 or 1</li> <li>Not suitable for multi-class classification</li> </ul> <h3 id="categorical-cross-entropy-loss">Categorical Cross-Entropy Loss</h3> <p>Tags: Classification</p> <p>What: The categorical cross-entropy loss is a loss function that is commonly used in multi-class classification problems. It is the negative log-likelihood of the true labels given the predicted probabilities. The categorical cross-entropy loss is defined as:</p> \[CCE = -\frac{1}{n} \sum_{i=1}^{n} \sum_{j=1}^{m} y_{ij} \log(\hat{y}_{ij})\] <p>where \(y_{ij}\) is either 0 or 1, and \(\hat{y}_{ij}\) is the predicted probability of the \(j\)-th class.</p> <p>Why: Naturally suited for multi-class classification tasks as it penalises model outputs from how divergent they are from the true labels. Itâ€™s interpretation is also intuitive as it is the negative log-likelihood of the true labels given the predicted probabilities.</p> <p>Pros:</p> <ul> <li>Effective for multi-class classification</li> <li>Differentiable and stable gradients</li> <li>Direct optimization of classification performance</li> </ul> <p>Cons:</p> <ul> <li>Sensitive to class imbalance</li> <li>Requires one-hot encoding of the true labels</li> <li>Numerical instability when the predicted probabilities are close to 0 or 1</li> </ul> <h3 id="sparse-categorical-cross-entropy-loss">Sparse Categorical Cross-Entropy Loss</h3> <p>Tags: Classification, Computer Vision</p> <p>What: Loss function similar to CCE but used when the true labels are not one-hot encoded.</p> <h3 id="hinge-loss">Hinge Loss</h3> <p>Tags: Classification</p> <p>What: The hinge loss is a loss function that is commonly used in binary classification problems to classify with maximum margin, mainly SVMs. It is the maximum of 0 and the difference between 1 and the predicted value times the true label. The hinge loss is defined as:</p> \[HL = \frac{1}{n} \sum_{i=1}^{n} \max(0, 1 - y_i \hat{y}_i)\] <p>where \(y_i\) is either -1 or 1, and \(\hat{y}_i\) is the predicted value. The loss penalizes the model when the predicted value is not in the correct direction of the true label. Even if the predicted value is correct, the loss is still penalized if the predicted value is not far enough from the decision boundary. An extension of the hinge loss is the squared hinge loss, which squares the difference between 1 and the predicted value times the true label.</p> <p>Why: Suited for classification tasks as it encourages the model to classify with maximum margin, which gives robust classification boundaries.</p> <p>Pros:</p> <ul> <li>Encourages the model to classify with maximum margin, gives robust classification boundaries</li> <li>Solution relies on the support vectors (sparse subset of the data), which makes the model more robust to outliers</li> </ul> <p>Cons:</p> <ul> <li>Non probabilistic, difficult to interpret</li> <li>Not easily extendable to multi-class classification</li> </ul> <h3 id="focal-loss">Focal Loss</h3> <p>Tags: Classification, Object Detection</p> <p>What: The focal loss is a loss function that is commonly used in binary classification problems. It is a modification of the binary cross-entropy loss that down-weights the contribution of well-classified examples. The focal loss is defined as:</p> \[FL = -\frac{1}{n} \sum_{i=1}^{n} \alpha_t (1 - \hat{y}_i)^\gamma \log(\hat{y}_i)\] <p>where \(\alpha_t\) is a hyperparameter that controls class imbalance (tuned using cross validation), \(\gamma\) is a hyperparameter that controls the down-weighting of well-classified examples. \(\gamma\) is typically set to 2.</p> <p>Why: The focal loss is a good loss function to use when the class imbalance is a concern. It down-weights the contribution of well-classified examples, which makes the model focus more on the hard examples.</p> <p>Pros:</p> <ul> <li>Effective for class imbalance</li> <li>Improved learning on difficult examples</li> <li>Can be used for multi-class classification</li> </ul> <p>Cons:</p> <ul> <li>Requires tuning of hyperparameters \(\alpha_t\) and \(\gamma\)</li> <li>performance is sensitive to these hyperparameters</li> <li>Similar effectiveness to CE when class imbalance is not a concern</li> </ul> <p>Links:</p> <ul> <li><a href="https://arxiv.org/pdf/1708.02002.pdf">Focal Loss for Dense Object Detection</a></li> </ul> <h3 id="kullback-leibler-divergence-kl-divergence">Kullback-Leibler Divergence (KL Divergence)</h3> <p>Tags: Generative Models, VAE, GAN</p> <p>What: The Kullback-Leibler divergence (KL divergence) is a loss function that is commonly used in generative models. It is a measure from information theory that measures how one probability distribution diverges from a second, approximate probability distribution. The KL divergence is defined as:</p> \[KL(P||Q) = \sum_{x \in \mathcal{X}} P(x) \log \frac{P(x)}{Q(x)}\] <p>where \(P\) is the true distribution and \(Q\) is the approximate distribution. It is not symmetric and is always non-negative. It is zero if and only if \(P\) and \(Q\) are the same distribution. It is infinite if the support of \(P\) is not contained in the support of \(Q\).</p>]]></content><author><name></name></author><category term="Loss Functions"/><summary type="html"><![CDATA[Short introduction to policy gradient methods]]></summary></entry><entry><title type="html">TDMPC-2</title><link href="https://shreyaspimpalgaonkar.github.io/blog/2024/TDMPC2/" rel="alternate" type="text/html" title="TDMPC-2"/><published>2024-03-08T00:00:00+00:00</published><updated>2024-03-08T00:00:00+00:00</updated><id>https://shreyaspimpalgaonkar.github.io/blog/2024/TDMPC2</id><content type="html" xml:base="https://shreyaspimpalgaonkar.github.io/blog/2024/TDMPC2/"><![CDATA[<p>TDMPC-2</p> <h2 id="algorithm">Algorithm</h2> <ol> <li>Log Train Metrics</li> </ol>]]></content><author><name></name></author><category term="RL"/><category term="MPC"/><summary type="html"><![CDATA[Using Temporal Difference for Model Predictive Control]]></summary></entry><entry><title type="html">Policy Gradient</title><link href="https://shreyaspimpalgaonkar.github.io/blog/2024/policy_gradient/" rel="alternate" type="text/html" title="Policy Gradient"/><published>2024-03-08T00:00:00+00:00</published><updated>2024-03-08T00:00:00+00:00</updated><id>https://shreyaspimpalgaonkar.github.io/blog/2024/policy_gradient</id><content type="html" xml:base="https://shreyaspimpalgaonkar.github.io/blog/2024/policy_gradient/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Policy gradient methods are a type of reinforcement learning algorithm that directly learn the policy of an agent. The policy is a mapping from states to actions, and the goal of the agent is to learn a policy that maximizes the expected return. Policy gradient methods are particularly useful in continuous action spaces, where the policy is a probability distribution over actions.</p> <p>Policy gradient based algorithms have become quite popular in recent years, and have been successfully applied to a wide range of problems, including robotics, games, and natural language processing (RLHF!).</p> <h2 id="objective">Objective</h2> <p>The objective of policy gradient methods is to maximize the expected return by adjusting the policy parameters. The expected return is defined as the expected sum of rewards over a trajectory, and the policy parameters are adjusted using the gradient of the expected return with respect to the policy parameters.</p> <p>The policy is typically parameterized by a neural network with parameters \(\theta\), and the gradient of the expected return with respect to the policy parameters is given by the policy gradient:</p> \[\nabla_{\theta} J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}} \left[ \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t|s_t) R(\tau) \right]\] <p>where \(\pi_{\theta}\) is the policy, \(\tau\) is a trajectory, \(s_t\) is the state at time \(t\), \(a_t\) is the action at time \(t\), and \(R(\tau)\) is the return of the trajectory.</p>]]></content><author><name></name></author><category term="RL"/><category term="Policy Gradient"/><summary type="html"><![CDATA[Short introduction to policy gradient methods]]></summary></entry><entry><title type="html">Interesting Links</title><link href="https://shreyaspimpalgaonkar.github.io/blog/2024/Interesting-Links/" rel="alternate" type="text/html" title="Interesting Links"/><published>2024-02-01T00:00:00+00:00</published><updated>2024-02-01T00:00:00+00:00</updated><id>https://shreyaspimpalgaonkar.github.io/blog/2024/Interesting-Links</id><content type="html" xml:base="https://shreyaspimpalgaonkar.github.io/blog/2024/Interesting-Links/"><![CDATA[<ul> <li>https://linacolucci.com/2018/09/tomato-method/</li> <li>https://www.youtube.com/watch?v=3LopI4YeC4I</li> </ul>]]></content><author><name></name></author><summary type="html"><![CDATA[]]></summary></entry></feed>